#!/usr/bin/env python3

# Usage: ./workbench --config config.yml

import os
import sys
import json
import csv
import logging
import datetime
import argparse
import mimetypes
from workbench_utils import *


def create():
    """Create new nodes via POST, and add media if there are any.
    """
    logging.info('"Create" task started using config file %s', args.config)
    input_csv = os.path.join(config['input_dir'], config['input_csv'])
    if os.path.exists(input_csv):
        # Store a dict of id_field: node IDs so we can add linked nodes.
        # This dict is not currently used but could be for things like
        # https://github.com/mjordan/islandora_workbench/issues/18.
        node_ids = dict()

        field_definitions = get_field_definitions(config)
        with open(input_csv) as csvfile:
            csv_data = csv.DictReader(csvfile, delimiter=config['delimiter'])
            csv_column_headers = csv_data.fieldnames

            node_endpoint = config['host'] + '/node?_format=json'

            for row in csv_data:
                row = clean_csv_values(row)
                # Add required fields.
                node = {
                    'type': [
                        {'target_id': config['content_type'],
                         'target_type': 'node_type'}
                    ],
                    'title': [
                        {'value': row['title']}
                    ]
                }

                # Add custom (non-required) CSV fields.
                required_fields = ['file', 'title']
                custom_fields = list(
                    set(csv_column_headers)-set(required_fields))
                for custom_field in custom_fields:
                    # Skip updating field if value is empty.
                    if len(row[custom_field]) == 0:
                        continue
                    # For (non-entity reference (text, etc.) fields.
                    if field_definitions[custom_field]['field_type'] != 'entity_reference':
                        # Cardinality is unlimited.
                        if field_definitions[custom_field]['cardinality'] == -1:
                            if config['subdelimiter'] in row[custom_field]:
                                field_values = []
                                subvalues = row[custom_field].split(config['subdelimiter'])
                                for subvalue in subvalues:
                                    field_values.append({'value': subvalue})
                                    node[custom_field] = field_values
                                else:
                                    node[custom_field] = [{'value': row[custom_field]}]
                            else:
                                node[custom_field] = [{'value': row[custom_field]}]
                        elif field_definitions[custom_field]['cardinality'] > 1:
                            if config['subdelimiter'] in row[custom_field]:
                                field_values = []
                                subvalues = row[custom_field].split(config['subdelimiter'])
                                # @todo: log fact that we're slicing off values.
                                subvalues = subvalues[:field_definitions[custom_field]['cardinality']]
                                for subvalue in subvalues:
                                    field_values.append({'value': subvalue})
                                    node[custom_field] = field_values
                            else:
                                node[custom_field] = [{'value': row[custom_field]}]
                        # Cardinality is 1.
                        else:
                            node[custom_field] = [{'value': row[custom_field]}]

                    # Entity reference fields: for taxonomy terms, target_type
                    # it's 'taxonomy_term'; for nodes, it's 'node_type'.
                    else:
                        if field_definitions[custom_field]['target_type'] == 'taxonomy_term':
                            target_type = 'taxonomy_term'
                        if field_definitions[custom_field]['target_type'] == 'node':
                            target_type = 'node_type'
                        # Cardinality is unlimited.
                        if field_definitions[custom_field]['cardinality'] == -1:
                            if config['subdelimiter'] in row[custom_field]:
                                field_values = []
                                subvalues = row[custom_field].split(config['subdelimiter'])
                                # @todo: log fact that we're slicing off values.
                                subvalues = subvalues[:field_definitions[custom_field]['cardinality']]
                                for subvalue in subvalues:
                                    field_values.append({'target_id': subvalue, 'target_type': target_type})
                                    node[custom_field] = field_values
                            else:
                                node[custom_field] = [
                                                      {'target_id': row[custom_field],
                                                       'target_type': target_type}]
                        elif field_definitions[custom_field]['cardinality'] > 1:
                            if config['subdelimiter'] in row[custom_field]:
                                field_values = []
                                subvalues = row[custom_field].split(config['subdelimiter'])
                                # @todo: log fact that we're slicing off values.
                                for subvalue in subvalues:
                                    field_values.append({'target_id': subvalue, 'target_type': target_type})
                                    # @todo: log fact that we're slicing off valudes.
                                    node[custom_field] = field_values[:field_definitions[custom_field]['cardinality']]
                            else:
                                node[custom_field] = [
                                                      {'target_id': row[custom_field],
                                                       'target_type': target_type}]
                        # Cardinality is 1.
                        else:
                            node[custom_field] = [
                                                  {'target_id': row[custom_field],
                                                   'target_type': target_type}]

                node_headers = {
                    'Content-Type': 'application/json'
                }
                node_endpoint = '/node?_format=json'
                node_response = issue_request(config, 'POST', node_endpoint, node_headers, node, None)
                node_uri = node_response.headers['location']
                if node_response.status_code == 201:
                    print("Node for '" + row['title'] +
                          "' created at " + node_uri + ".")
                    logging.info("Node for %s created at %s.", row['title'], node_uri)

                # Get ID of newly created node so we can use it for linking
                # child nodes, etc.
                if 'id_field' in config:
                    id_field = row[config['id_field']]
                    if node_response.status_code == 201:
                        node_ids[id_field] = node_uri.rsplit('/', 1)[-1]

                # If there is no media file, move on to the next CSV row.
                if len(row['file']) == 0:
                    print("+No media for " + node_uri + " created since " +
                          "its 'file' field in the CSV is empty.")
                    logging.warning("No media for %s created since its " +
                                    "'file' field in the CSV is empty.", node_uri)
                    continue

                # If there is a media file, add it.
                file_path = os.path.join(config['input_dir'], row['file'])
                mimetype = mimetypes.guess_type(file_path)
                media_type = 'file'
                if mimetype[0] in image_mimetypes:
                    media_type = 'image'
                if mimetype[0] in audio_mimetypes:
                    media_type = 'audio'
                if mimetype[0] in video_mimetypes:
                    media_type = 'video'

                if node_response.status_code == 201 and len(row['file']) != 0:
                    media_endpoint_path = ('/media/' +
                                           media_type +
                                           '/' + str(config['media_use_tid']))
                    media_endpoint = node_uri + media_endpoint_path
                    location = config['drupal_filesystem'] + os.path.basename(row['file'])
                    media_headers = {
                        'Content-Type': mimetype[0],
                        'Content-Location': location
                    }
                    binary_data = open(os.path.join(
                        config['input_dir'], row['file']), 'rb')
                    media_response = issue_request(config, 'PUT', media_endpoint, media_headers, '', binary_data)
                    allowed_binary_response_codes = [201, 204]
                    if media_response.status_code in allowed_binary_response_codes:
                        print('+' + media_type.title() + " media for " +
                              row['file'] + " created.")
                        logging.info("%s media for %s created.", media_type.title(), row['file'])
                if node_response.status_code == 201 and len(row['file']) == 0:
                    print('+ No file specified in CSV for ' + row['title'])


def update():
    """Update nodes via PATCH. Note that PATCHing replaces the target field,
       so if we are adding an additional value to a multivalued field, we need
       to include the existing value in our PATCH.
    """
    input_csv = os.path.join(config['input_dir'], config['input_csv'])
    if os.path.exists(input_csv):
        field_definitions = get_field_definitions(config)
        with open(input_csv) as csvfile:
            csv_data = csv.DictReader(csvfile, delimiter=config['delimiter'])
            csv_column_headers = csv_data.fieldnames

            for row in csv_data:
                row = clean_csv_values(row)
                if not ping_node(config, row['node_id']):
                    print("Node " + row['node_id'] + " not found or not " +
                          "accessible, skipping update.")
                    continue

                # Add the target_id field.
                node = {
                    'type': [
                        {'target_id': config['content_type']}
                    ]
                }

                node_field_values = get_node_field_values(config, row['node_id'])

                # Add custom (non-required) fields.
                required_fields = ['node_id']
                custom_fields = list(
                    set(csv_column_headers)-set(required_fields))
                for custom_field in custom_fields:
                    # Skip updating field if value is empty.
                    if len(row[custom_field]) == 0:
                        continue
                    # Non entity reference fields (text/integer, etc.):
                    # if the field's cardinality is 1, replace field value;
                    # if it's not, get the field's current value and append
                    # the new value.
                    if field_definitions[custom_field]['field_type'] != 'entity_reference':
                        if field_definitions[custom_field]['cardinality'] == 1:
                            node[custom_field] = [{'value': row[custom_field]}]
                        elif field_definitions[custom_field]['cardinality'] > 1:
                            # Append to existing values.
                            if config['subdelimiter'] in row[custom_field]:
                                field_values = []
                                subvalues = row[custom_field].split(config['subdelimiter'])
                                # @todo: log fact that we're slicing off values.
                                subvalues = subvalues[:field_definitions[custom_field]['cardinality']]
                                for subvalue in subvalues:
                                    field_values.append({'value': subvalue})
                                    node[custom_field] = node_field_values[custom_field] + field_values
                            else:
                                node[custom_field] = node_field_values[custom_field] + [{'value': row[custom_field]}]
                        # Cardinatlity is unlimited.
                        else:
                            # Append to existing values.
                            if config['subdelimiter'] in row[custom_field]:
                                field_values = []
                                subvalues = row[custom_field].split(config['subdelimiter'])
                                for subvalue in subvalues:
                                    field_values.append({'value': subvalue})
                                    node[custom_field] = node_field_values[custom_field] + field_values
                            else:
                                node[custom_field] = node_field_values[custom_field] + [{'value': row[custom_field]}]

                    # For taxonomy terms, target_type is 'taxonomy_term';
                    # for nodes, it's 'node_type'.
                    else:
                        if field_definitions[custom_field]['target_type'] == 'taxonomy_term':
                            target_type = 'taxonomy_term'
                        if field_definitions[custom_field]['target_type'] == 'node':
                            target_type = 'node_type'

                        if field_definitions[custom_field]['cardinality'] == 1:
                            node[custom_field] = [
                                {'target_id': row[custom_field],
                                 'target_type': target_type}]
                        elif field_definitions[custom_field]['cardinality'] > 1:
                            # Append to existing values.
                            if config['subdelimiter'] in row[custom_field]:
                                field_values = []
                                subvalues = row[custom_field].split(config['subdelimiter'])
                                # @todo: log fact that we're slicing off values.
                                subvalues = subvalues[:field_definitions[custom_field]['cardinality']]
                                for subvalue in subvalues:
                                    field_values.append({'target_id': subvalue, 'target_type': target_type})
                                    node[custom_field] = node_field_values[custom_field] + field_values
                            else:
                                node[custom_field] = node_field_values[custom_field] + [
                                    {'target_id': row[custom_field],
                                     'target_type': 'taxonomy_term'}]
                        # Cardinality is unlimited.
                        else:
                            # Append to existing values.
                            if config['subdelimiter'] in row[custom_field]:
                                field_values = []
                                subvalues = row[custom_field].split(config['subdelimiter'])
                                for subvalue in subvalues:
                                    field_values.append({'target_id': subvalue, 'target_type': target_type})
                                    node[custom_field] = node_field_values[custom_field] + field_values
                            else:
                                node[custom_field] = node_field_values[custom_field] + [
                                    {'target_id': row[custom_field],
                                     'target_type': 'taxonomy_term'}]

                node_endpoint = config['host'] + '/node/' + row['node_id'] + '?_format=json'
                node_headers = {
                    'Content-Type': 'application/json'
                }
                node_response = issue_request(config, 'PATCH', node_endpoint, node_headers, node)

                if node_response.status_code == 200:
                    print("Node for " + config['host'] + '/node/' +
                          row['node_id'] + " updated.")
                    logging.info("Node for %s updated.", config['host'] + '/node/' + row['node_id'])


def delete():
    """Delete nodes.
    """
    input_csv = os.path.join(config['input_dir'], config['input_csv'])
    if os.path.exists(input_csv):
        with open(input_csv) as csvfile:
            csv_data = csv.DictReader(csvfile)
            csv_column_headers = csv_data.fieldnames

            for row in csv_data:
                row = clean_csv_values(row)
                if not ping_node(config, row['node_id']):
                    print("Node " + row['node_id'] + " not found or not " +
                          "accessible, skipping delete.")
                    continue

                node_endpoint = config['host'] + '/node/' + str(row['node_id']) + '?_format=json'
                node_response = issue_request(config, 'DELETE', node_endpoint)
                if node_response.status_code == 204:
                    print("Node " + config['host'] + '/node/' +
                          str(row['node_id']) + " deleted.")
                    logging.info("Node %s deleted.", config['host'] +
                                 '/node/' + row['node_id'])


def add_media():
    """Add media to existing nodes using PUT.
    """
    input_csv = os.path.join(config['input_dir'], config['input_csv'])
    if os.path.exists(input_csv):
        with open(input_csv) as csvfile:
            csv_data = csv.DictReader(csvfile, delimiter=config['delimiter'])
            csv_column_headers = csv_data.fieldnames

            for row in csv_data:
                row = clean_csv_values(row)
                if not ping_node(config, row['node_id']):
                    print("Node " + row['node_id'] + " not found or not " +
                          "accessible, skipping adding media.")
                    continue

                file_path = os.path.join(config['input_dir'], row['file'])
                mimetype = mimetypes.guess_type(file_path)
                media_type = 'file'
                if mimetype[0] in image_mimetypes:
                    media_type = 'image'
                if mimetype[0] in audio_mimetypes:
                    media_type = 'audio'
                if mimetype[0] in video_mimetypes:
                    media_type = 'video'

                node_json_url = config['host'] + '/node/' + row['node_id'] + '?_format=json'
                node_uri = config['host'] + '/node/' + row['node_id']
                node_response = issue_request(config, 'GET', node_json_url)
                if node_response.status_code == 200:
                    media_endpoint_path = ('/media/' +
                                           media_type + '/' +
                                           str(config['media_use_tid']))
                    media_endpoint = node_uri + media_endpoint_path
                    location = config['drupal_filesystem'] + os.path.basename(row['file'])
                    media_headers = {
                        'Content-Type': mimetype[0],
                        'Content-Location': location
                    }
                    binary_data = open(
                        os.path.join(config['input_dir'], row['file']), 'rb')
                    media_response = issue_request(config, 'PUT', media_endpoint, media_headers, '', binary_data)
                    allowed_binary_response_codes = [201, 204]
                    if media_response.status_code in allowed_binary_response_codes:
                        print(media_type.title() + " media for " + row['file'] + " created and added to " + node_uri)
                        logging.info("%s media for %s created and added to %s.", media_type.title(), row['file'], node_uri)


# Main program logic.

parser = argparse.ArgumentParser()
parser.add_argument(
    '--config',
    help='Configuration file to use.')
parser.add_argument(
    '--check',
    help='Check input data and exit without creating/updating/etc.', action='store_true')
args = parser.parse_args()

# TIFFs and JP2s are 'file', as is everything else not in these lists.
image_mimetypes = ['image/jpeg', 'image/png', 'image/gif']
audio_mimetypes = ['audio/mpeg3', 'audio/wav', 'audio/aac']
video_mimetypes = ['video/mp4']
mimetypes.init()

config = set_config_defaults(args)
logging.basicConfig(
    filename=config['log_file_path'],
    level=logging.INFO,
    filemode=config['log_file_mode'],
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%d-%b-%y %H:%M:%S')

if config['check']:
    check_input(config, args)

if config['task'] == 'create':
    create()
if config['task'] == 'update':
    update()
if config['task'] == 'delete':
    delete()
if config['task'] == 'add_media':
    add_media()
