#!/usr/bin/env python3

# Usage: ./workbench --config config.yml

import os
import sys
import json
import argparse
import csv
import mimetypes
import requests
from ruamel.yaml import YAML

parser = argparse.ArgumentParser()
parser.add_argument(
    '--config',
    help='Configuration file to use.')
parser.add_argument(
    '--check',
    help='Check input data and exit without creating/updating/etc.', action='store_true')

mimetypes.init()
# TIFFs and JP2s are 'file'.
image_mimetypes = ['image/jpeg', 'image/png', 'image/gif']
audio_mimetypes = ['audio/mpeg3', 'audio/wav', 'audio/aac']
video_mimetypes = ['video/mp4']


def set_config_defaults():
    """Convert the YAML configuration data into an array for easy use.
    """
    args = parser.parse_args()
    yaml = YAML()

    # Check existence of configuration file.
    if not os.path.exists(args.config):
        sys.exit('Error: Configuration file ' + args.config + 'not found.')

    config_file_contents = open(args.config).read()
    config_data = yaml.load(config_file_contents)

    config = {}
    for k, v in config_data.items():
        config[k] = v

    if 'subdelimiter' not in config:
        config['subdelimiter'] = '|'

    if args.check:
        config['check'] = True
    else:
        config['check'] = False

    return config


def get_field_definitions():
    """Query Drupal to get fields definitions.
    """
    field_config_url = config['host'] + '/jsonapi/field_storage_config/field_storage_config'
    headers = {'Accept': 'Application/vnd.api+json'}
    response = requests.get(
        field_config_url,
        auth=(config['username'], config['password']),
        headers=headers
    )

    if response.status_code == 200:
        field_config = json.loads(response.text)
        field_definitions = {}
        for item in field_config['data']:
            field_name = item['attributes']['field_name']
            if 'target_type' in item['attributes']['settings']:
                target_type = item['attributes']['settings']['target_type']
            else:
                target_type = None
            field_definitions[field_name] = {
                'field_type': item['attributes']['field_storage_config_type'],
                'cardinality': item['attributes']['cardinality'],
                'target_type': target_type}
        return field_definitions


def check_input():
    """Validate the config file and input data.
    """
    # First, check the config file.
    tasks = ['create', 'update', 'delete', 'add_media']
    joiner = ', '
    if config['task'] not in tasks:
        sys.exit('Error: "task" in your configuration file must be one of "create", "update", "delete", "add_media".')

    config_keys = list(config.keys())
    config_keys.remove('check')

    # Dealing with optional config keys. If you introduce a new
    # optional key, add it to this list. Note that optional
    # keys are not validated.
    optional_config_keys = ['id_field', 'subdelimiter']
    for optional_config_key in optional_config_keys:
        if optional_config_key in config_keys:
            config_keys.remove(optional_config_key)

    # Check for presence of required config keys.
    if config['task'] == 'create':
        create_options = ['task', 'host', 'username', 'password',
                          'input_dir', 'input_csv', 'media_use_tid',
                          'drupal_filesystem', 'model_tid', 'delimiter']
        if not set(config_keys) == set(create_options):
            sys.exit('Error: Please check your config file for required values: ' + joiner.join(create_options))
    if config['task'] == 'update':
        update_options = ['task', 'host', 'username', 'password',
                          'input_dir', 'input_csv', 'delimiter']
        if not set(config_keys) == set(update_options):
            sys.exit('Error: Please check your config file for required values: ' + joiner.join(update_options))
    if config['task'] == 'delete':
        delete_options = ['task', 'host', 'username', 'password',
                          'input_dir', 'input_csv']
        if not set(config_keys) == set(delete_options):
            sys.exit('Error: Please check your config file for required values: ' + joiner.join(delete_options))
    if config['task'] == 'add_media':
        add_media_options = ['task', 'host', 'username', 'password',
                             'input_dir', 'input_csv', 'media_use_tid',
                             'drupal_filesystem', 'delimiter']
        if not set(config_keys) == set(add_media_options):
            sys.exit('Error: Please check your config file for required values: ' + joiner.join(add_media_options))
    print('OK, configuration file has all required values (did not check for optional values).')

    # Test host and credentials.
    jsonapi_url = config['host'] + '/jsonapi/field_storage_config/field_storage_config'
    headers = {'Accept': 'Application/vnd.api+json'}
    try:
        response = requests.get(
            jsonapi_url,
            auth=(config['username'], config['password']),
            headers=headers
        )
        response.raise_for_status()
    except requests.exceptions.TooManyRedirects as error:
        print(error)
        sys.exit(1)
    except requests.exceptions.RequestException as error:
        print(error)
        sys.exit(1)

    # JSON:API returns a 200 but an empty 'data' array if credentials are bad.
    if response.status_code == 200:
        field_config = json.loads(response.text)
        if field_config['data'] == []:
            sys.exit('Error: ' + config['host'] + ' does not recognize the username/password combination you have provided.'    )
        else:
            print('OK, ' + config['host'] + ' is accessible.')

    # Check existence of CSV file.
    input_csv = os.path.join(config['input_dir'], config['input_csv'])
    if os.path.exists(input_csv):
        print('OK, CSV file ' + input_csv + ' found.')
    else:
        sys.exit('Error: CSV file ' + input_csv + 'not found.')

    # Check column headers in CSV file.
    with open(input_csv) as csvfile:
        csv_data = csv.DictReader(csvfile, delimiter=config['delimiter'])
        csv_column_headers = csv_data.fieldnames

        if config['task'] == 'create':
            if 'file' not in csv_column_headers:
                sys.exit('Error: For "create" tasks, your CSV file must contain a "file" column.')
            if 'title' not in csv_column_headers:
                sys.exit('Error: For "create" tasks, your CSV file must contain a "title" column.')
            field_definitions = get_field_definitions()
            drupal_fieldnames = []
            for drupal_fieldname in field_definitions:
                drupal_fieldnames.append(drupal_fieldname)
            if 'title' in csv_column_headers:
                csv_column_headers.remove('title')
            if 'file' in csv_column_headers:
                csv_column_headers.remove('file')
            if 'node_id' in csv_column_headers:
                csv_column_headers.remove('node_id')
            for csv_column_header in csv_column_headers:
                if csv_column_header not in drupal_fieldnames:
                    sys.exit('Error: CSV column header "' + csv_column_header + '" does not appear to match any Drupal field names.')
            print('OK, CSV column headers match Drupal field names.')

        if config['task'] == 'update':
            if 'node_id' not in csv_column_headers:
                sys.exit('Error: For "update" tasks, your CSV file must contain a "node_id" column.')
            field_definitions = get_field_definitions()
            drupal_fieldnames = []
            for drupal_fieldname in field_definitions:
                drupal_fieldnames.append(drupal_fieldname)
            if 'title' in csv_column_headers:
                csv_column_headers.remove('title')
            if 'file' in csv_column_headers:
                sys.exit('Error: CSV column header "file" is not allowed in update tasks.')
            if 'node_id' in csv_column_headers:
                csv_column_headers.remove('node_id')
            for csv_column_header in csv_column_headers:
                if csv_column_header not in drupal_fieldnames:
                    sys.exit('Error: CSV column header "' + csv_column_header + '" does not appear to match any Drupal field names.')
            print('OK, CSV column headers match Drupal field names.')

        if config['task'] == 'delete':
            if 'node_id' not in csv_column_headers:
                sys.exit('Error: For "delete" tasks, your CSV file must contain a "node_id" column.')
        if config['task'] == 'add_media':
            if 'node_id' not in csv_column_headers:
                sys.exit('Error: For "add_media" tasks, your CSV file must contain a "node_id" column.')
            if 'file' not in csv_column_headers:
                sys.exit('Error: For "add_media" tasks, your CSV file must contain a "file" column.')

        # Check for existence of files listed in the 'files' column.
        if config['task'] == 'create' or config['task'] == 'add_media':
            # Opening the CSV again is easier than copying the unmodified csv_data variable. Because Python.
            with open(input_csv) as csvfile:
                file_check_csv_data = csv.DictReader(csvfile, delimiter=config['delimiter'])
                for file_check_row in file_check_csv_data:
                    file_path = os.path.join(config['input_dir'], file_check_row['file'])
                    if not os.path.exists(file_path):
                        sys.exit('Error: File ' + file_path + ' identified in CSV "file" column not found.')
            print('OK, files named in the CSV "file" column are all present.')

    # If nothing has failed by now, exit with a positive message.
    print("Configuration and input data appear to be valid.")
    sys.exit(0)


config = set_config_defaults()
if config['check']:
    check_input()


def create():
    """Create new nodes via POST.
    """
    input_csv = os.path.join(config['input_dir'], config['input_csv'])
    if os.path.exists(input_csv):
        # Store a dict of id_field: node IDs so we can add linked nodes.
        # This dict is not currently used but could be for things like
        # https://github.com/mjordan/islandora_workbench/issues/18.
        node_ids = dict()

        field_definitions = get_field_definitions()
        with open(input_csv) as csvfile:
            csv_data = csv.DictReader(csvfile, delimiter=config['delimiter'])
            csv_column_headers = csv_data.fieldnames

            node_endpoint = config['host'] + '/node?_format=json'

            for row in csv_data:
                # Add required fields.
                node = {
                    'type': [
                        {'target_id': 'islandora_object',
                         'target_type': 'node_type'}
                    ],
                    'title': [
                        {'value': row['title']}
                    ],
                    'field_model': [
                        {'target_id': str(config['model_tid']),
                         'target_type': 'taxonomy_term'}
                    ]
                }

                # Add custom (non-required) CSV fields.
                required_fields = ['file', 'title']
                custom_fields = list(
                    set(csv_column_headers)-set(required_fields))
                for custom_field in custom_fields:
                    # Skip updating field if value is empty.
                    if len(row[custom_field]) == 0:
                        continue
                    # For (non-entity reference (text, etc.) fields.
                    if field_definitions[custom_field]['field_type'] != 'entity_reference':
                        # Cardinality is unlimited.
                        if field_definitions[custom_field]['cardinality'] == -1:
                            if config['subdelimiter'] in row[custom_field]:
                                field_values = []
                                subvalues = row[custom_field].split(config['subdelimiter'])
                                for subvalue in subvalues:
                                    field_values.append({'value': subvalue})
                                    node[custom_field] = field_values
                                else:
                                    node[custom_field] = [{'value': row[custom_field]}]        
                        elif field_definitions[custom_field]['cardinality'] > 1:
                            if config['subdelimiter'] in row[custom_field]:
                                field_values = []
                                subvalues = row[custom_field].split(config['subdelimiter'])
                                # @todo: log fact that we're slicing off values.
                                subvalues = subvalues[:field_definitions[custom_field]['cardinality']]
                                for subvalue in subvalues:
                                    field_values.append({'value': subvalue})
                                    node[custom_field] = field_values
                            else:
                                node[custom_field] = [{'value': row[custom_field]}]
                        # Cardinality is 1.
                        else:
                            node[custom_field] = [{'value': row[custom_field]}]                            

                    # Entity referernce fields: for taxonomy terms, target_type
                    # is'taxonomy_term'; for nodes, it's 'node_type'.
                    else:
                        if field_definitions[custom_field]['target_type'] == 'taxonomy_term':
                            target_type = 'taxonomy_term'
                        if field_definitions[custom_field]['target_type'] == 'node':
                            target_type = 'node_type'
                        # Cardinality is unlimited.
                        if field_definitions[custom_field]['cardinality'] == -1:
                            if config['subdelimiter'] in row[custom_field]:
                                field_values = []
                                subvalues = row[custom_field].split(config['subdelimiter'])
                                # @todo: log fact that we're slicing off values.
                                subvalues = subvalues[:field_definitions[custom_field]['cardinality']]
                                for subvalue in subvalues:
                                    field_values.append({'target_id': subvalue, 'target_type': target_type})
                                    node[custom_field] = field_values
                            else:
                                node[custom_field] = [
                                                      {'target_id': row[custom_field],
                                                       'target_type': target_type}]
                        elif field_definitions[custom_field]['cardinality'] > 1:
                            if config['subdelimiter'] in row[custom_field]:
                                field_values = []
                                subvalues = row[custom_field].split(config['subdelimiter'])
                                # @todo: log fact that we're slicing off values.
                                for subvalue in subvalues:
                                    field_values.append({'target_id': subvalue, 'target_type': target_type})
                                    # @todo: log fact that we're slicing off valudes.
                                    node[custom_field] = field_values[:field_definitions[custom_field]['cardinality']]                                    
                            else:
                                node[custom_field] = [
                                                      {'target_id': row[custom_field],
                                                       'target_type': target_type}]
                        # Cardinality is 1.
                        else:
                            node[custom_field] = [
                                                  {'target_id': row[custom_field],
                                                   'target_type': target_type}]

                node_headers = {
                    'Content-Type': 'application/json'
                }
                node_response = requests.post(
                    node_endpoint,
                    auth=(config['username'], config['password']),
                    json=node, headers=node_headers)
                node_uri = node_response.headers['location']
                if node_response.status_code == 201:
                    print("Node for '" + row['title'] +
                          "' created at " + node_uri + ".")

                # Get ID of newly created node in case we want to use it
                # later for linking child nodes, etc.
                if 'id_field' in config:
                    id_field = row[config['id_field']]
                    if node_response.status_code == 201:
                        node_ids[id_field] = node_uri.rsplit('/', 1)[-1]

                file_path = os.path.join(config['input_dir'], row['file'])
                mimetype = mimetypes.guess_type(file_path)
                media_type = 'file'
                if mimetype[0] in image_mimetypes:
                    media_type = 'image'
                if mimetype[0] in audio_mimetypes:
                    media_type = 'audio'
                if mimetype[0] in video_mimetypes:
                    media_type = 'video'

                if node_response.status_code == 201 and len(row['file']) != 0:
                    media_endpoint_path = ('/media/' +
                                           media_type +
                                           '/' + str(config['media_use_tid']))
                    media_endpoint = node_uri + media_endpoint_path
                    location = config['drupal_filesystem'] + os.path.basename(row['file'])
                    media_headers = {
                        'Content-Type': mimetype[0],
                        'Content-Location': location
                    }
                    binary_data = open(os.path.join(
                        config['input_dir'], row['file']), 'rb')
                    media_response = requests.put(
                        media_endpoint,
                        auth=(config['username'], config['password']),
                        data=binary_data,
                        headers=media_headers)
                    allowed_binary_response_codes = [201, 204]
                    if media_response.status_code in allowed_binary_response_codes:
                        print('+' + media_type.title() + " media for " +
                              row['file'] + " created.")
                if node_response.status_code == 201 and len(row['file']) == 0:
                    print('+ No file specified in CSV for ' + row['title'])


def update():
    """Note that PATCHing replaces the target field, so if we are adding
       an additional value to a multivalued field, we need to include
       the existing value in our PATCH.
    """
    input_csv = os.path.join(config['input_dir'], config['input_csv'])
    if os.path.exists(input_csv):
        field_definitions = get_field_definitions()
        with open(input_csv) as csvfile:
            csv_data = csv.DictReader(csvfile, delimiter=config['delimiter'])
            csv_column_headers = csv_data.fieldnames

            for row in csv_data:
                # Add the type/target_id field.
                node = {
                    'type': [
                        {'target_id': 'islandora_object'}
                    ]
                }

                node_field_values = get_node_field_values(row['node_id'])

                # Add custom (non-required) fields.
                required_fields = ['node_id']
                custom_fields = list(
                    set(csv_column_headers)-set(required_fields))
                for custom_field in custom_fields:
                    # Skip updating field if value is empty.
                    if len(row[custom_field]) == 0:
                        continue                                           
                    # Non entity reference fields (text/integer, etc.):
                    # if the field's cardinality is 1, replace field value;
                    # if it's not, get the field's current value and append
                    # the new value.
                    if field_definitions[custom_field]['field_type'] != 'entity_reference':
                        if field_definitions[custom_field]['cardinality'] == 1:
                            node[custom_field] = [{'value': row[custom_field]}]
                        elif field_definitions[custom_field]['cardinality'] > 1:
                            # Append to existing values.
                            if config['subdelimiter'] in row[custom_field]:
                                field_values = []
                                subvalues = row[custom_field].split(config['subdelimiter'])
                                # @todo: log fact that we're slicing off values.
                                subvalues = subvalues[:field_definitions[custom_field]['cardinality']]
                                for subvalue in subvalues:
                                    field_values.append({'value': subvalue})
                                    node[custom_field] = node_field_values[custom_field] + field_values
                            else:
                                node[custom_field] = node_field_values[custom_field] + [{'value': row[custom_field]}]
                        # Cardinatlity is unlimited.
                        else:
                            # Append to existing values.
                            if config['subdelimiter'] in row[custom_field]:
                                field_values = []
                                subvalues = row[custom_field].split(config['subdelimiter'])
                                for subvalue in subvalues:
                                    field_values.append({'value': subvalue})
                                    node[custom_field] = node_field_values[custom_field] + field_values
                            else:
                                node[custom_field] = node_field_values[custom_field] + [{'value': row[custom_field]}]

                    # For taxonomy terms, target_type is 'taxonomy_term';
                    # for nodes, it's 'node_type'.
                    else:
                        if field_definitions[custom_field]['target_type'] == 'taxonomy_term':
                            target_type = 'taxonomy_term'
                        if field_definitions[custom_field]['target_type'] == 'node':
                            target_type = 'node_type'

                        if field_definitions[custom_field]['cardinality'] == 1:
                            node[custom_field] = [
                                {'target_id': row[custom_field],
                                 'target_type': target_type}]
                        elif field_definitions[custom_field]['cardinality'] > 1:
                            # Append to existing values.
                            if config['subdelimiter'] in row[custom_field]:                            
                                field_values = []
                                subvalues = row[custom_field].split(config['subdelimiter'])
                                # @todo: log fact that we're slicing off values.
                                subvalues = subvalues[:field_definitions[custom_field]['cardinality']]
                                for subvalue in subvalues:
                                    field_values.append({'target_id': subvalue, 'target_type': target_type})
                                    node[custom_field] = node_field_values[custom_field] + field_values
                            else:
                                node[custom_field] = node_field_values[custom_field] + [
                                    {'target_id': row[custom_field],
                                     'target_type': 'taxonomy_term'}]                            
                        # Cardinality is unlimited.                                 
                        else:
                            # Append to existing values.
                            if config['subdelimiter'] in row[custom_field]:                            
                                field_values = []
                                subvalues = row[custom_field].split(config['subdelimiter'])
                                for subvalue in subvalues:
                                    field_values.append({'target_id': subvalue, 'target_type': target_type})
                                    node[custom_field] = node_field_values[custom_field] + field_values
                            else:
                                node[custom_field] = node_field_values[custom_field] + [
                                    {'target_id': row[custom_field],
                                     'target_type': 'taxonomy_term'}]

                node_endpoint = config['host'] + '/node/' + row['node_id'] + '?_format=json'
                node_headers = {
                    'Content-Type': 'application/json'
                }
                node_response = requests.patch(
                    node_endpoint,
                    auth=(config['username'], config['password']),
                    json=node, headers=node_headers)

                if node_response.status_code == 200:
                    print("Node for " + config['host'] + '/node/' +
                          row['node_id'] + " updated.")


def delete():
    """Delete nodes.
    """
    input_csv = os.path.join(config['input_dir'], config['input_csv'])
    if os.path.exists(input_csv):
        with open(input_csv) as csvfile:
            csv_data = csv.DictReader(csvfile)
            csv_column_headers = csv_data.fieldnames

            for row in csv_data:
                node_endpoint = config['host'] + '/node/' + str(row['node_id']) + '?_format=json'
                node_response = requests.delete(
                    node_endpoint,
                    auth=(config['username'], config['password']))
                if node_response.status_code == 204:
                    print("Node " + config['host'] + '/node/' +
                          str(row['node_id']) + " deleted.")


def add_media():
    """Add media to existing nodes using PUT.
    """
    input_csv = os.path.join(config['input_dir'], config['input_csv'])
    if os.path.exists(input_csv):
        with open(input_csv) as csvfile:
            csv_data = csv.DictReader(csvfile, delimiter=config['delimiter'])
            csv_column_headers = csv_data.fieldnames

            for row in csv_data:
                file_path = os.path.join(config['input_dir'], row['file'])
                mimetype = mimetypes.guess_type(file_path)
                media_type = 'file'
                if mimetype[0] in image_mimetypes:
                    media_type = 'image'
                if mimetype[0] in audio_mimetypes:
                    media_type = 'audio'
                if mimetype[0] in video_mimetypes:
                    media_type = 'video'

                node_json_url = config['host'] + '/node/' + row['node_id'] + '?_format=json'
                node_uri = config['host'] + '/node/' + row['node_id']
                node_response = requests.get(
                    node_json_url,
                    auth=(config['username'], config['password']))
                if node_response.status_code == 200:
                    media_endpoint_path = ('/media/' +
                                           media_type + '/' +
                                           str(config['media_use_tid']))
                    media_endpoint = node_uri + media_endpoint_path
                    location = config['drupal_filesystem'] + os.path.basename(row['file'])
                    media_headers = {
                        'Content-Type': mimetype[0],
                        'Content-Location': location
                    }
                    binary_data = open(
                        os.path.join(config['input_dir'], row['file']), 'rb')
                    media_response = requests.put(
                        media_endpoint,
                        auth=(config['username'], config['password']),
                        data=binary_data,
                        headers=media_headers)
                    allowed_binary_response_codes = [201, 204]
                    if media_response.status_code in allowed_binary_response_codes:
                        print(media_type.title() + " media for " + row['file'] + " created and added to " + node_uri)


def get_node_field_values(nid):
    """Get a node's field data so we can use it during PATCH updates,
       which replace a field's values.
    """
    node_url = config['host'] + '/node/' + nid + '?_format=json'
    response = requests.get(
        node_url, auth=(config['username'], config['password'])
    )
    node_fields = json.loads(response.text)
    return node_fields


if config['task'] == 'create':
    create()
if config['task'] == 'update':
    update()
if config['task'] == 'delete':
    delete()
if config['task'] == 'add_media':
    add_media()
